
启动hdfs  ，
sh /opt/module/hadoop-2.7.2/sbin/start-dfs.sh jps
启动zookp
/opt/module/zookeeper-3.4.13/bin/zkServer.sh start

 ./zkServer.sh ../conf/zoo.cfg 
./zkServer.sh start

后台启动kafka 
sh /opt/module/kafka/bin/kafka-server-start.sh  /opt/module/kafka/config/server.properties &

cd  /opt/module/kafka/bin 
./kafka-console-producer.sh --broker-list hadoop20:9092 --topic first

./kafka-console-consumer.sh --bootstrap-server hadoop20:9092 --topic first

自动生成日志命令 
 nohup /opt/module/logs/ceshi.sh 2>&1 >/dev/null &
 echo '' > /opt/module/logs/log/web.log 

启动flume 
cd /opt/module/flume/bin/
./flume-ng agent --conf ../conf/ --name agent --conf-file ../job/flume_kafka.conf -Dflume.root.logger=INFO,console
./flume-ng agent -c ./ -f ../conf/customInterceptor3.conf -n customInterceptor | grep INFO

spark 启动spark
sh /opt/module/spark2.2/sbin/start-all.sh  jps  worker

yarn 启动yarn
 cd /opt/module/hadoop-2.7.2/sbin/ 
 ./start-yarn.sh    jps 
 ./mr-jobhistory-daemon.sh start historyserver jps JobHistoryServe

华为平台命令
cd 




先启动zookpeeper 
在启动hdfs
cd  /opt/module/spark-2.4.0-bin-hadoop2.7/sbin
# ./start-all.sh 
jps
./spark-shell  
./spark-shell --master spark://hadoop20:7077 --total-executor-cores 1 --executor-memory 512m

#HDFS服务器
http://10.0.2.60:50070/explorer.html#/
#hadoop集群
http://10.0.2.61:8088/cluster
#hadoop集群运行日志
http://10.0.2.61:19888/jobhistory